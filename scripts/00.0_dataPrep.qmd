---
title: "Data Prep"
format:
  html:
    embed-resources: true
editor: visual
bibliography: references.bib
---

# Goals and introduction

Prepare data for the movement/social analysis. This involves loading the data from Movebank and cleaning it.

# Setup

```{r load-packages}
library(vultureUtils)
library(sf)
library(tidyverse)
library(move)
library(feather)
library(readxl)
library(elevatr)
library(here)
library(furrr)
future::plan(future::multisession, workers = 5) # this will last for the whole R session.

# function to downsample data to an arbitrary number of minutes, assuming df contains only one individual and timestamp column is called "timestamp"
down <- function(df, mins){
  df <- dplyr::arrange(df, timestamp)
  howmuch <- paste(mins, "minute", sep = " ")
  rounded <- lubridate::round_date(df$timestamp, howmuch)
  keep <- !duplicated(rounded)
  return(df[keep,])
}
```

# Processing for all data

## Download data from Movebank

### Authenticate to Movebank

```{r}
base::load(here::here("movebankCredentials", "pw.Rda"))
MB.LoginObject <- move::movebankLogin(username = "kaijagahm", password = pw)
rm(pw)
```

### INPA data

Data collected by Ohad Hatzofe and team with GPS loggers deployed for their study.

```{r download-inpa}
inpa <- move::getMovebankData(study = 6071688, login = MB.LoginObject, removeDuplicatedTimestamps = TRUE)
inpa <- methods::as(inpa, "data.frame")
inpa <- inpa %>%
  mutate(dateOnly = lubridate::ymd(substr(timestamp, 1, 10)),
         year = as.numeric(lubridate::year(timestamp))) %>%
  filter(lubridate::ymd(dateOnly) >= lubridate::ymd("2020-09-01"), lubridate::ymd(dateOnly) <= lubridate::ymd("2023-09-15")) # cut this off at the same point as the ornitela data
write_feather(inpa, here::here("data", "dataPrep", "inpa.feather"))
```

### Ornitela data

Data collected by the TAU-UCLA partnership under the NSF-BSF grant. Ornitela tags.

```{r download-ornitela}
# Ornitela: download data from movebank (just a subset of the times for now)
minDate <- "2020-09-01 00:00"
maxDate <- "2023-09-15 11:59"
ornitela <- vultureUtils::downloadVultures(loginObject = MB.LoginObject, removeDup = T, dfConvert = T, quiet = T, dateTimeStartUTC = minDate, dateTimeEndUTC = maxDate)
write_feather(ornitela, here::here("data", "dataPrep", "ornitela.feather"))
```

## Join INPA and Ornitela data

```{r load-inpa-ornitela}
inpa <- read_feather(here::here("data", "dataPrep", "inpa.feather"))
ornitela <- read_feather(here::here("data", "dataPrep", "ornitela.feather"))
```

```{r join-inpa-ornitela}
#| eval: false

# Will there be a problem joining these?
length(names(ornitela)[!(names(ornitela) %in% names(inpa))]) # all names in "ornitela" are found in inpa
length(names(inpa)[!(names(inpa) %in% names(ornitela))]) # many are not found in "ornitela" that are found in inpa

# Add the dataset names so we can keep track of where the data comes from
inpa_tojoin <- inpa[,names(ornitela)] %>%
  mutate(dataset = "inpa")
ornitela <- ornitela %>%
  mutate(dataset = "ornitela")

# Join the two datasets
joined0 <- bind_rows(inpa_tojoin, ornitela)
dim(joined0)

rm(inpa)
rm(inpa_tojoin)
rm(ornitela)
```

```{r join-nili-ids}
ww <- read_excel(here("data/raw/whoswho_vultures_20230920_new.xlsx"), 
                 sheet = "all gps tags")
ww_tojoin <- ww %>% dplyr::select(Nili_id, Movebank_id) %>% distinct() # pull out just the names columns, nothing else, and remove any duplicates

# Prepare for join: are there any individuals in the `local_identifier` column of `joined0` that don't appear in the `Movebank_id` column of `ww_tojoin`?
problems <- joined0 %>% filter(!(local_identifier %in% ww_tojoin$Movebank_id)) %>% pull(local_identifier) %>% unique()
problems #let's check these against the who's who and see if we can make some reasonable changes.

## Fixes:
# Typo in the Movebank_id column of the who's who:
ww_tojoin <- ww_tojoin %>% mutate(Movebank_id = case_when(Movebank_id == "A65 Whiite" ~ "A65 White",
                                                          .default = Movebank_id))
# Fixes to joined0:
joined0 <- joined0 %>%
  mutate(local_identifier = case_when(local_identifier == "E86 White" ~ "E86",
                                      local_identifier == "E88 White" ~ "E88w",
                                      .default = local_identifier))

## Look for any remaining problems:
problems <- joined0 %>% filter(!(local_identifier %in% ww_tojoin$Movebank_id)) %>% pull(local_identifier) %>% unique()
problems #going to fix both of these afterward. E66 isn't listed in the Who's who at all, so we'll just call it "E66" in the Nili_id. The other one, Y01>T60 W, I've manually determined is Nili_id "tammy".
 
# join by movebank ID
joined <- left_join(joined0, ww_tojoin, 
                        by = c("local_identifier" = "Movebank_id"))
joined <- joined %>%
  mutate(Nili_id = case_when(is.na(Nili_id) & local_identifier == "E66 White" ~ "E66",
                             is.na(Nili_id) & local_identifier == "Y01>T60 W" ~ "tammy",
                             .default = Nili_id))

# Are there any remaining NA's for Nili_id?
nas <- joined %>% filter(is.na(Nili_id)) %>% pull(local_identifier) %>% unique()
length(nas) # yay, no more!

write_feather(joined, here("data/dataPrep/joined.feather"))
```

## Downsample to 5-minute frequency

This data is just WAY too huge to handle now that we have the higher-frequency fixes from the summer/fall of 2023. Going to downsample it a bit.

```{r}
# split `joined` by `Nili_id`
lst <- joined %>% group_split(Nili_id)

# apply the function to each of the ~200 individuals
interval_5min <- map(lst, ~down(df = .x, mins = 5), .progress = T)
interval_5min <- purrr::list_rbind(interval_5min)
write_feather(interval_5min, here("data/dataPrep/interval_5min.feather"))
nrow(joined)
nrow(interval_5min) # okay, that removed a lot!
```

## Remove invalid periods

### Hospital/pre-determined invalid

Removing known invalid periods, such as times when the vultures were in the hospital or the tags were known not to be attached to birds. These times are derived from a separate Excel sheet.

```{r load-hospital}
interval_5min <- read_feather(here::here("data/dataPrep/interval_5min.feather"))
periods_to_remove <- read_excel(here::here("data/raw/whoswho_vultures_20230920_new.xlsx"), sheet = "periods_to_remove")
```

```{r remove-periods}
removed_periods <- vultureUtils::removeInvalidPeriods(dataset = interval_5min, periodsToRemove = periods_to_remove)
save(removed_periods, file = here("data/dataPrep/removed_periods.Rda"))
```

## Clean the data based on filters for invalid data points (see `vultureUtils` package code for details)

```{r}
# warning: this takes a long time and uses a lot of memory!
cleaned <- vultureUtils::cleanData(dataset = removed_periods,
                     precise = F,
                     longCol = "location_long",
                     latCol = "location_lat",
                     idCol = "Nili_id",
                     report = F)
rm(interval_5min)
gc()
save(cleaned, file = here("data/dataPrep/cleaned.Rda"))

(nrow(removed_periods)-nrow(cleaned))/nrow(removed_periods) # removed less than 1% of the points.
```

## Remove time spent in capture cages

```{r}
capture_sites <- read.csv(here("data/raw/capture_sites.csv"))
carmel <- read.csv(here("data/raw/all_captures_carmel_2010-2021.csv"))
removed_captures <- removeCaptures(data = cleaned, 
                                       captureSites = capture_sites, 
                                       AllCarmelDates = carmel, 
                                       distance = 500, idCol = "Nili_id")

(nrow(removed_captures)-nrow(cleaned))/nrow(cleaned) # did not remove any data. Should probably check on this...
save(removed_captures, file = here("data/dataPrep/removed_captures.Rda"))
```

## Attach birth year and sex information

Note: the actual calculation of age happens later on, after cleaning the data, for some reason.

```{r attach-age-sex}
#| message: false
load(here("data/dataPrep/removed_captures.Rda"))
age_sex <- read_excel(here::here("data", "raw", "whoswho_vultures_20230920_new.xlsx"), 
                      sheet = "all gps tags")[,1:35] %>%
  dplyr::select(Nili_id, birth_year, sex) %>%
  distinct()

with_age_sex <- removed_captures %>%
  dplyr::select(-c("sex")) %>%
  left_join(age_sex, by = "Nili_id")
nrow(with_age_sex) == nrow(removed_captures) # should be TRUE
length(unique(with_age_sex$Nili_id)) #200

mutate(age = year - birth_year, #XXX consider moving this up to the age/sex section. Why is it happening here?
         sex = case_when(sex == "f/m" ~ "u",
                         TRUE ~ sex)) %>%

# tidy up
rm(removed_captures)
rm(age_sex)
gc()
```

## Mask data

Here, we set the `inMaskThreshold` to 30 days overall. If we wanted 30 days per season, we would have to do that differently (e.g. split the seasons before this step, not after). But I think that gets covered later in the data cleaning anyway.

```{r region-masking}
mask <- sf::st_read(here("data/raw/CutOffRegion.kml"))

# XXX TEST
test <- sample_n(with_age_sex, 50000)
reMask <- T # XXX INCORPORATE THIS INTO THE FUNCTION!!
masked_test <- vultureUtils::inMaskFilter(dataset = test, mask = mask, inMaskThreshold = 0.33)
# XXX END TEST

# Region masking
data_masked <- vultureUtils::inMaskFilter(dataset = with_age_sex, mask = mask, inMaskThreshold = 0.33, crs = "WGS84")$dataset # this takes a long time!!

# Fix time zone so dates make sense ---------------------------------------
## Overwrite the dateOnly column from the new times
data_masked <- data_masked %>%
  mutate(timestampIsrael = lubridate::with_tz(timestamp, tzone = "Israel"),
         dateOnly = lubridate::date(timestampIsrael),
         month = lubridate::month(dateOnly),
         year = lubridate::year(dateOnly))
save(data_masked, file = here("data/dataPrep/data_masked.Rda")) # just the df #XXX fix


# tidy up
rm(mask)
rm(with_age_sex)
gc()
```

## Split into seasons

Season definitions, and code to split the data into seasons, come from Marta.

```{r split-seasons}
load(here("data/dataPrep/data_masked.Rda")) # XXX fix
data_masked <- sf::st_drop_geometry(data_masked) # to make it faster to work on

# Split into Marta's 3 seasons --------------------------------------------
with_seasons <- data_masked %>% mutate(month = lubridate::month(timestampIsrael),
                day = lubridate::day(timestampIsrael),
                year = lubridate::year(timestampIsrael)) %>%
  mutate(season = case_when(((month == 12 & day >= 15) | 
                              (month %in% 1:4) | 
                              (month == 5 & day < 15)) ~ "breeding",
                            ((month == 5 & day >= 15) | 
                               (month %in% 6:8) | 
                               (month == 9 & day < 15)) ~ "summer",
                            .default = "fall")) %>%
  filter(dateOnly != "2023-09-15") %>% # remove September 15 2023, because it turns out the season boundaries are supposed to be non-inclusive and this is the latest one.
  mutate(seasonUnique = case_when(season == "breeding" & month == 12 ~
                                    paste(as.character(year + 1), season, sep = "_"),
                                  .default = paste(as.character(year), season, sep = "_"))) %>%
  mutate(seasonUnique = factor(seasonUnique, levels = c("2020_summer", "2020_fall", "2021_breeding", "2021_summer", "2021_fall", "2022_breeding", "2022_summer", "2022_fall", "2023_breeding", "2023_summer")),
         season = factor(season, levels = c("breeding", "summer", "fall")))

# Add ages (based on season, which is why this goes here rather than above)
with_seasons <- with_seasons %>%
  mutate(age = year - birth_year) %>%
  mutate(age = ifelse(season == "breeding" & month %in% c(1, 12), age + 1, age)) %>%
  group_by(Nili_id, year, month) %>%
  mutate(age = ifelse(month == 2, min(age) + 1, age)) %>%
  mutate(age_group = case_when(age == 0 ~ "juv",
                               age >= 1 & age <= 4 ~ "sub",
                               age >= 5 ~ "adult",
                               .default = NA)) %>%
  ungroup()

# REMOVE SUMMER 2020--we don't need to use it
with_seasons <- with_seasons %>%
  filter(seasonUnique != "2020_summer")
length(unique(with_seasons$seasonUnique)) # should be 9

# Separate the seasons -----------------------------
seasons_list <- with_seasons %>%
  group_by(seasonUnique) %>%
  group_split(.keep = T)
length(seasons_list) # should be 9
# extract the season names in case we need a separate vector at some point.
season_names <- map_chr(seasons_list, ~as.character(.x$seasonUnique[1])) # ok good, these are in the right order
length(season_names) # should also be 9
save(seasons_list, file = here::here("data", "dataPrep", "seasons_list.Rda"))
save(season_names, file = here::here("data", "dataPrep", "season_names.Rda"))

# tidy up
rm(cleaned)
rm(seasons_list)
rm(with_seasons)
rm(season_names)
gc()
```

## Restrict to southern population

Based on previous investigations for the 2022 breeding and non-breeding seasons, have found that a good cutoff for southern vs. non-southern is 3550000 (UTM 36N, <https://epsg.io/32636)>

```{r}
load(here("data/dataPrep/seasons_list.Rda"))
load(here("data/dataPrep/season_names.Rda"))
```

```{r}
#| eval: false
# Transform to SF object, so we can get centroids
seasons_sf <- map(seasons_list, ~.x %>%
                   sf::st_as_sf(coords = c("location_long", "location_lat"), remove = F) %>%
                   sf::st_set_crs("WGS84") %>%
                   sf::st_transform(32636))

## Get centroids, so we can see who's "southern" for that season.
centroids <- map(seasons_sf, ~.x %>%
                   group_by(Nili_id) %>%
                   summarize(geometry = st_union(geometry)) %>%
                   st_centroid())

## Examine a histogram of centroid latitudes 
#walk(centroids, ~hist(st_coordinates(.x)[,2])) # looks like 3550000 is generally a good cutoff point here.

## Get southern individuals for each season, so we can filter the data
southern_indivs <- map(centroids, ~.x %>%
                        filter(st_coordinates(.)[,2] < 3550000) %>%
                        pull(Nili_id))

## Remove individuals not in the south
removed_northern <- map2(.x = seasons_list, 
                         .y = southern_indivs, ~.x 
                         %>% filter(Nili_id %in% .y))
save(removed_northern, file = here("data/dataPrep/removed_northern.Rda"))

# tidy up
rm(seasons_list)
rm(season_names)
rm(seasons_sf)
rm(centroids)
rm(southern_indivs)
rm(removed_northern)
rm(df)
gc()
```

## Identify individuals with low fix rate

Some of the INPA vultures were tracked only once an hour, or some other low fix rate. However, determining which birds these are is tricky because even the vultures that should have a 10-minute fix rate do not always have points every 10 minutes, e.g. due to lack of reception or low tag battery.\
\
Therefore, we have to identify **low fix rate (lfr) individuals**, which are those that *never* have a 10-minute fix rate. To do this, we will find the mode (most common) fix frequency for each individual on each day. Then, we will identify individuals who never have observations at 10-minute intervals. We will keep any that theoretically have a 10-minute fix rate (i.e. their observation frequency was 10 minutes at some point), even if in practice they often have much less frequent observations.

Because tags can be put on and off and the fix rate can change, I'm calculating the fix rate on a per-season basis (which is why I didn't put this step earlier in the pipeline–it had to come after the season splitting).

```{r}
load(here::here("data/dataPrep/removed_northern.Rda"))
load(here::here("data/dataPrep/season_names.Rda"))
```

```{r}
#| eval: false
# function to calculate mode (most common) fix rate
Mode <- function(x) { 
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

# Calculate daily modes and add them to the dataset (using the function defined above)
with_modes <- map(removed_northern, ~.x %>%
                 group_by(dateOnly, Nili_id) %>%
                 mutate(diff = as.numeric(difftime(lead(timestamp), timestamp, units = "mins"))) %>%
                 group_by(dateOnly, Nili_id) %>%
                 mutate(mode = Mode(round(diff))) %>%
                 ungroup())

# Identify individuals that never have a daily mode of 10 minutes
low_fix_rate_indivs <- map(with_modes, ~.x %>%
                        sf::st_drop_geometry() %>%
                        group_by(Nili_id) %>%
                        summarize(minmode = min(mode, na.rm = T)) %>%
                        filter(minmode > 10) %>%
                        pull(Nili_id) %>%
                        unique())
names(low_fix_rate_indivs) <- season_names

save(low_fix_rate_indivs, 
     file = here::here("data", "dataPrep", "low_fix_rate_indivs.Rda"))
# Don't need to save with_modes because we've already used this step to identify low fix rate individuals.

# Tidy up
rm(removed_northern)
rm(season_names)
rm(low_fix_rate_indivs)
rm(with_modes)
gc()
```

## Remove low fix rate individuals

```{r}
load(here::here("data", "dataPrep", "removed_northern.Rda")) 
load(here::here("data", "dataPrep", "low_fix_rate_indivs.Rda"))
```

```{r}
removed_lfr <- map2(removed_northern, low_fix_rate_indivs,                     
                    ~.x %>% filter(!(Nili_id %in% .y))) 
save(removed_lfr, file = here::here("data", "dataPrep", "removed_lfr.Rda"))  
# tidy up 
rm(removed_northern) 
rm(low_fix_rate_indivs) 
gc()
```

## Remove high-frequency fixes (downsample for social)

```{r}
load(here::here("data", "dataPrep", "removed_lfr.Rda"))
```

```{r}
lst <- map(removed_lfr, ~.x %>% group_split(Nili_id))
downsampled_10min_forSocial <- map(lst, ~{
  purrr::list_rbind(map(.x, ~down(df = .x, mins = 10), .progress = T))
}, .progress = T)

# Add information on # of days tracked for each individual
downsampled_10min_forSocial <- map(downsampled_10min_forSocial, ~.x %>% group_by(Nili_id) %>% 
                       mutate(daysTracked = length(unique(dateOnly))))
save(downsampled_10min_forSocial, file = here::here("data/dataPrep/downsampled_10min_forSocial.Rda"))

# tidying
rm(removed_lfr)
gc()
```

### Data for social analysis

The next step will be to remove individuals with too few points per day or too few days tracked. But we don't want those indivs removed for the *social* analysis, since those things will be accounted for with SRI and all individuals make up important parts of the social fabric. So when we do the social analysis, we will use `removed_lfr` and not anything further along.

## Get roosts before removing night points

We are going to need to get nightly roost locations for all of the vultures. We have to do this now because getting roost locations requires using points that occur at night, which we will eventually want to remove in order to do the movement analysis.

```{r}
load(here::here("data/dataPrep/downsampled_10min_forSocial.Rda"))
```

```{r}
#| eval: false
# Get roosts for each season ----------------------------------------------
roosts <- map(downsampled_10min_forSocial, ~vultureUtils::get_roosts_df(df = .x, id = "Nili_id"), .progress = T)
roosts <- roosts %>%
  map(., ~st_as_sf(.x, crs = "WGS84", 
                   coords = c("location_long", "location_lat"), 
                   remove = F), .progress = T)
save(roosts, file = here::here("data", "dataPrep", "roosts.Rda"))

# Tidy up
rm(downsampled_10min_forSocial)
gc()
```

# Further processing for movement analysis

## Remove nighttime points

```{r}
load(here::here("data/dataPrep/removed_lfr.Rda"))
load(here::here("data/dataPrep/season_names.Rda"))
```

```{r}
#| eval: false
# Remove nighttime points -------------------------------------------------
removed_nighttime <- map(removed_lfr, ~{
  times <- suncalc::getSunlightTimes(date = unique(lubridate::date(.x$timestamp)),
                                     lat = 31.434306, lon = 34.991889, # XXX revisit this!
                                     keep = c("sunrise", "sunset")) %>%
    dplyr::select("dateOnly" = date, sunrise, sunset)

  .x <- .x %>%
    dplyr::mutate(dateOnly = lubridate::ymd(dateOnly)) %>%
    dplyr::left_join(times, by = "dateOnly") %>%
    dplyr::mutate(daylight = ifelse(timestamp >= sunrise & timestamp <= sunset, "day", "night")) %>%
    dplyr::filter(daylight == "day")
}, .progress = T)

save(removed_nighttime, file = here::here("data/dataPrep/removed_nighttime.Rda"))

# tidy up
rm(removed_lfr)
rm(season_names)
gc()
```

## Remove vulture-days with too few points per day

```{r}
load(here::here("data/dataPrep/removed_nighttime.Rda"))
```

```{r}
#| eval: false
# Calculate number of points per day for each individual
# XXX START HERE can't find battery charge percent
ppd <- map_dfr(removed_nighttime, ~.x %>%
                 st_drop_geometry() %>%
                 group_by(seasonUnique, season, Nili_id, dateOnly) %>%
                 summarize(n = n(),
                           minBatt = min(battery_charge_percent, na.rm = T)) %>% 
                 ungroup(), .progress = T)

# Sometimes, fewer points per day is due to low battery charge percent. Let's examine the relationship between battery charge and number of points per day.
ppd %>%
  filter(n < 100) %>% # only interested in lower numbers of points per day
  ggplot(aes(x = minBatt, y = n))+
  geom_point()+
  facet_wrap(~seasonUnique)+
  theme_classic()

# What if we decide to exclude based on the combination of battery charge and points per day, with the minimum battery threshold at 50% and the points threshold at 10?

# That is, if the minimum battery threshold was <= 50, a bird must have > 10 points on a given day for that day to be included. But if the minimum battery threshold was never less than 50%, it's okay to keep the ones with fewer points per day, since we know it was due to some reason other than battery charge.

excl_batt <- 50
excl_pts <- 10
## min battery charge
ppd %>%
  filter(n < 100) %>% # only interested in the lower numbers
  mutate(remove = case_when(minBatt <= excl_batt & n <= excl_pts ~ T,
                            TRUE ~ F)) %>%
  ggplot(aes(x = minBatt, y = n))+
  geom_point(aes(col = remove))+
  facet_wrap(~seasonUnique) +
  scale_color_manual(values = c("black", "red")) +
  theme_classic()

## Keep days with more than 10 points, or with fewer than 10 if the minimum battery charge that day is >50%
removed_lowppd <- map(removed_nighttime, ~.x %>%
                 group_by(Nili_id, dateOnly) %>%
                 filter(n() >= 10 | (n() < 10 & 
                                       min(battery_charge_percent, 
                                           na.rm = T) > 50)) %>%
                 ungroup())

save(removed_lowppd, file = here::here("data", "dataPrep", "removed_lowppd.Rda"))

# tidying up
rm(removed_nighttime)
rm(removed_northern)
rm(ppd)
rm(excl_batt)
rm(excl_pts)
gc()
```

## Remove individuals with too few high-ppd days tracked

Now that we've removed days with too few points per day, we are left with only "valid" movement days for each individual. Now we want to exclude *individuals* (rather than individual-days) if the individual has too few valid days of movement data to accurately calculate movement metrics.

Somewhat arbitrarily, we are removing individuals that are tracked for **fewer than 30 days in a given season.**

```{r}
load(here::here("data", "dataPrep", "removed_lowppd.Rda"))
load(here::here("data", "dataPrep", "season_names.Rda"))
```

```{r}
#| eval: false

# How long are the seasons?
season_durations <- map_dbl(removed_lowppd, ~max(.x$dateOnly)-min(.x$dateOnly))
names(season_durations) <- season_names
season_durations

# Include only individuals with enough days tracked -----------------------
indivsToKeep <- map(removed_lowppd, ~.x %>%
                       st_drop_geometry() %>%
                       group_by(Nili_id) %>%
                       summarize(nDaysTracked = 
                                   length(unique(dateOnly))) %>%
                       filter(nDaysTracked >= 30) %>%
                       pull(Nili_id), .progress = T)

## remove those individuals not tracked for enough days
removed_too_few_days <- map2(.x = removed_lowppd, 
                             .y = indivsToKeep, ~.x %>% 
                filter(Nili_id %in% .y))

save(removed_too_few_days, file = here::here("data", "dataPrep", "removed_too_few_days.Rda"))
save(season_durations, file = here::here("data", "dataPrep", "season_durations.Rda"))

# tidy up
rm(removed_lowppd)
rm(season_names)
rm(season_durations)
rm(indivsToKeep)
rm(removed_too_few_days)
gc()
```

## Calculate flight altitude

```{r}
load(here::here("data", "dataPrep", "removed_too_few_days.Rda"))
```

```{r}
#| eval: false
# Get elevation rasters ---------------------------------------------------
sf <- map(removed_too_few_days, ~st_as_sf(.x, coords = c("location_lat", "location_long"), remove = F, crs = "WGS84"))
elevs_z10 <- map(sf, ~elevatr::get_elev_raster(.x , z = 10), .progress = T)
save(elevs_z10, file = here::here("data", "dataPrep", "elevs_z10.Rda"))
save(sf, file = here::here("data", "dataPrep", "sf.Rda"))
```

```{r}
load(here::here("data", "dataPrep", "elevs_z10.Rda"))
load(here::here("data", "dataPrep", "sf.Rda"))
```

```{r}
#| eval: false
groundElev_z10 <- map2(elevs_z10, sf, ~raster::extract(x = .x, y = .y), .progress = T) # have to convert the crs because the raster from elevatr is in WGS84.

## use elevation rasters to calculate height above ground level
with_altitudes <- map2(.x = removed_too_few_days, .y = groundElev_z10, 
                ~.x %>% mutate(groundElev = .y) %>%
                  mutate(height_above_ground = height_above_msl-groundElev,
                         height_above_ground = case_when(height_above_ground < 0 ~ 0,
                                                         TRUE ~ height_above_ground)))

# There should be no change in the number of rows
all(map2_lgl(removed_too_few_days, with_altitudes, ~nrow(.x) == nrow(.y)))

save(with_altitudes, file = here::here("data", "dataPrep", "with_altitudes.Rda"))

# tidy up
rm(removed_too_few_days)
rm(sf)
rm(elevs_z10)
rm(groundElev_z10)
rm(with_altitudes)
gc()
```

## Remove high-frequency fixes

We already removed individuals that had too low a fix rate. We are left with individuals that had a fix rate of 10 minutes or more frequent. But occasionally, there were periods of high-frequency sampling. This might mess up our movement calculations, so let's downsample the data so that any points that were sampled at a frequency much higher than 1 fix/10 minutes will be reduced to the same frequency as the others.

```{r}
load(here::here("data", "dataPrep", "with_altitudes.Rda"))
load(here::here("data", "dataPrep", "season_names.Rda"))
```

```{r}
lst_2 <- map(with_altitudes, ~.x %>% group_split(Nili_id))
downsampled_10min <- map(lst_2, ~{
  purrr::list_rbind(map(.x, ~down(df = .x, mins = 10), .progress = T))
}, .progress = T)

# Add information on # of days tracked for each individual
downsampled_10min <- map(downsampled_10min, ~.x %>% group_by(Nili_id) %>% 
                       mutate(daysTracked = length(unique(dateOnly))))
save(downsampled_10min, file = here::here("data/dataPrep/downsampled_10min.Rda"))

# tidying
rm(with_altitudes)
rm(downsampled_10min)
gc()
```

```{r}
#| eval: false
# Examine the number of rows that get removed by the downsampling at different stages
load(here::here("data", "dataPrep", "removed_lfr.Rda"))
load(here::here("data", "dataPrep", "downsampled_10min_forSocial.Rda"))
load(here::here("data", "dataPrep", "with_altitudes.Rda"))
load(here::here("data", "dataPrep", "downsampled_10min_forSocial.Rda"))

df <- data.frame(seasonUnique = factor(season_names, levels =season_names), removed_lfr = map_dbl(removed_lfr, nrow), downsampled_10min_forSocial = map_dbl(downsampled_10min_forSocial, nrow), with_altitudes = map_dbl(with_altitudes, nrow), downsampled_10min = map_dbl(downsampled_10min, nrow)) %>% pivot_longer(-seasonUnique, names_to = "stage", values_to = "rows") %>% mutate(stage = factor(stage, levels = unique(stage)))

df %>% ggplot(aes(x = stage, y = rows, col = seasonUnique, group = seasonUnique))+geom_line()+theme_minimal()
```

This looks crazy, but it's actually correct. Note that the stages aren't actually linear– the `removed_lfr` –\> `downsampled_10min_forSocial` pathway is a separate offshoot. `downsampled_10min_forSocial` doesn't then get passed into the further cleaning for movement data; rather, it goes straight to the social analysis while `removed_lfr` passes along to the further cleaning. In 2023 summer, there were a lot of high-resolution fixes, so this matters a whole lot. I didn't notice the effects of the downsampling much before because the other seasons just weren't too big, but this new season is an order of magnitude larger, which makes the downsampling essential.

```{r}
#| eval: false

# Let's make sure the downsampling actually worked
testseason <- downsampled_10min[[1]]
with_ints <- testseason %>%
  group_by(Nili_id, dateOnly) %>%
  arrange(timestamp, .by_group = T) %>%
  mutate(int = difftime(lead(timestamp), timestamp, units = "mins"))
with_ints %>% 
  select(Nili_id, dateOnly, timestamp, int) %>%
  ggplot(aes(x = as.numeric(int), col = Nili_id))+
  geom_density()+
  theme_minimal()+
  theme(legend.position = "none") # Good, none of them are less than 0.

# Do we have any less than 10 minutes?
with_ints %>% filter(as.numeric(int) < 10) # yes, there are a lot that are less than 10 minutes

# What about less than 9 minutes?
with_ints %>% filter(as.numeric(int) < 9) # okay, so something in my code is not working to filter out the really short intervals. What's happening here?

# Oh... the way I did it actually guarantees this to happen. I cut the times into groups and then took the first one for each group. So if the first one for the previous group happened to be at the end of that group and then the first one of the next group happened to be at the beginning of that group, we can end up with arbitrarily small time intervals.

# Now, I kind of don't care, since the main point here was just to get rid of the points. But I wonder if there's a different way to do this.
```

I just read up on subsampling vs. thinning in [@gupte2022]. It seems like I've reinvented the wheel here. What I'm doing is subsampling (choosing one point from each time interval), as opposed to aggregation (summarizing all the points within a given time interval, e.g. taking the mean). They both have their faults; for example, aggregation adds gps error (which can be measured, but still) while subsampling risks selecting outlier points just by chance but does preserve the location error structure without further effort. They note that you need to do the rest of the cleaning first before doing the subsampling, which is exactly what I've done, so yay!

There is a function in `atlastools` for thinning the data, either by aggregation or subsampling. I'm going to test it and see if it's faster.

I tried out the atlastools function and it was quite buggy and not necessarily faster, so I'm going to stick with my own function. Can still cite this paper for a general guideline of how to do it though.

```{r}
# library(atlastools)
# library(tictoc)
# testseason <- removed_lfr[[1]] %>% mutate(x = "location_long", y = "location_lat") %>% select(x, y, timestamp, Nili_id, dateOnly) %>% filter(!is.na(timestamp), !is.na(x), !is.na(y), !is.na(Nili_id), !is.na(Nili_id)) %>% mutate(time = as.numeric(timestamp))
# tic()
# atl_thinned <- atl_thin_data(data = testseason,
#               interval = 600,
#               id_columns = c("Nili_id", "dateOnly"),
#               method = "subsample")
# toc()
```

Okay, so in sum, I am *not* worried about there being a few really brief time intervals remaining in my data. For the AKDE's, I'll want to use a 10-minute `dt` value to avoid applying the shortest interval over the entire dataset. But for proceeding, it's okay to have these results of the downsampling stand.
